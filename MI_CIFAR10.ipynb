{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict, OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "    \n",
    "def categorical_cross_entropy(output, target):\n",
    "    num_classes = 10\n",
    "    epsilon = 10e-8\n",
    "        \n",
    "    output = torch.clamp(output, epsilon, 1. - epsilon)\n",
    "    target = target.reshape(target.shape[0],1)\n",
    "    one_hot_target = (target == torch.arange(num_classes).reshape(1, num_classes)).float()\n",
    "    return torch.mean(-torch.sum(one_hot_target * torch.log(output), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "\n",
    "# convert data to a normalized torch.FloatTensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.CIFAR10('data', train=True,\n",
    "                              download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10('data', train=False,\n",
    "                             download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)\n",
    "\n",
    "full_data_loader = torch.utils.data.DataLoader(test_data, batch_size=10000, \n",
    "    num_workers=num_workers)\n",
    "\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for creating NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # convolutional layer (sees 32x32x3 image tensor)\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        # convolutional layer (sees 16x16x16 tensor)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        # convolutional layer (sees 8x8x32 tensor)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # linear layer (64 * 4 * 4 -> 500)\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 500)\n",
    "        # linear layer (500 -> 10)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        # dropout layer (p=0.25)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        activations = [ ]\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        activations.append(x.view(-1, 32 * 32 * 16).data.numpy())\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        activations.append(x.view(-1, 16 * 16 * 32 ).data.numpy())\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        activations.append(x.view(-1, 8 * 8 * 64).data.numpy())\n",
    "        x = self.pool(x)\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 1st hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        activations.append(x.view(-1, 500).data.numpy())\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 2nd hidden layer, with relu activation function\n",
    "        #x = F.log_softmax(self.fc2(x))\n",
    "        x = F.softmax(self.fc2(x))\n",
    "        activations.append(x.view(-1, 10).data.numpy())\n",
    "        return x, activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for saving activations, computing MI and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kde\n",
    "import simplebinmi\n",
    "\n",
    "class MI_Plate():\n",
    "    def __init__(self, n_epochs=100, DO_LOWER=False, DO_BINNED=False, PLOT_LAYERS=None):\n",
    "        super(MI_Plate, self).__init__()\n",
    "        \n",
    "        self.n_epochs = n_epochs\n",
    "        self.DO_LOWER = DO_LOWER\n",
    "        self.DO_BINNED = DO_BINNED\n",
    "        self.PLOT_LAYERS = PLOT_LAYERS\n",
    "    \n",
    "    def save_activations(self):\n",
    "        \n",
    "        if not os.path.exists('activations'):\n",
    "            print(\"Making directory\", 'activations')\n",
    "            os.mkdir('activations')\n",
    "            \n",
    "        model = Net()\n",
    "\n",
    "        if train_on_gpu:\n",
    "            model.cuda()\n",
    "\n",
    "        #criterion=nn.NLLLoss()\n",
    "\n",
    "        optimizer=optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            # monitor training loss\n",
    "            train_loss = 0.0\n",
    "\n",
    "            ###################\n",
    "            # train the model #\n",
    "            ###################\n",
    "            model.train()\n",
    "\n",
    "            for data, target in train_loader:\n",
    "                # clear the gradients of all optimized variables\n",
    "                optimizer.zero_grad()\n",
    "                # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                output, _ = model(data)\n",
    "                # calculate the loss\n",
    "                loss = categorical_cross_entropy(output, target)\n",
    "                # backward pass: compute gradient of the loss with respect to model parameters\n",
    "                loss.backward()\n",
    "                # perform a single optimization step (parameter update)\n",
    "                optimizer.step()\n",
    "                # update running training loss\n",
    "                train_loss += loss.item()#*data.size(0)\n",
    "\n",
    "            # print training statistics \n",
    "            # calculate average loss over an epoch\n",
    "            train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "                epoch+1, \n",
    "                train_loss\n",
    "                ))\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            for data, _ in full_data_loader:\n",
    "                _, activations = model(data)\n",
    "            \n",
    "            if epoch < 20:       # Log for all first 20 epochs\n",
    "                fname = 'activations' + \"/epoch%08d\"% epoch\n",
    "                print(\"Saving\", fname)\n",
    "                with open(fname, 'wb') as f:\n",
    "                    pickle.dump([activations, epoch], f, pickle.HIGHEST_PROTOCOL)\n",
    "            elif (epoch < 100) and (epoch % 5 == 0):    # Then for every 5th epoch\n",
    "                fname = 'activations' + \"/epoch%08d\"% epoch\n",
    "                print(\"Saving\", fname)\n",
    "                with open(fname, 'wb') as f:\n",
    "                    pickle.dump([activations, epoch], f, pickle.HIGHEST_PROTOCOL)\n",
    "            elif (epoch < 200) and (epoch % 10 == 0):    # Then every 10th\n",
    "                fname = 'activations' + \"/epoch%08d\"% epoch\n",
    "                print(\"Saving\", fname)\n",
    "                with open(fname, 'wb') as f:\n",
    "                    pickle.dump([activations, epoch], f, pickle.HIGHEST_PROTOCOL)\n",
    "            elif (epoch % 100 == 0):                # Then every 100th\n",
    "                fname = 'activations' + \"/epoch%08d\"% epoch\n",
    "                print(\"Saving\", fname)\n",
    "                with open(fname, 'wb') as f:\n",
    "                    pickle.dump([activations, epoch], f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "        \n",
    "    def compute_MI(self, noise_variance=1e-1):\n",
    "        \n",
    "        MAX_EPOCHS = self.n_epochs\n",
    "        self.noise_variance = noise_variance\n",
    "        \n",
    "        def entropy_func_upper(x):\n",
    "            return kde.entropy_estimator_kl(x,self.noise_variance)\n",
    "        \n",
    "        def entropy_func_lower(x):\n",
    "            return kde.entropy_estimator_bd(x,self.noise_variance)\n",
    "\n",
    "        # nats to bits conversion factor\n",
    "        nats2bits = 1.0/np.log(2) \n",
    "\n",
    "\n",
    "        # Save indexes of tests data for each of the output classes\n",
    "        saved_labelixs = {}\n",
    "        for data, targets in full_data_loader:\n",
    "            trg = targets.numpy()\n",
    "        for i in range(10):\n",
    "            saved_labelixs[i] = trg == i\n",
    "\n",
    "        labelprobs = np.mean(np.eye(10)[test_data.targets], axis=0)\n",
    "\n",
    "        measures = OrderedDict()\n",
    "        measures['relu'] = {}\n",
    "        #measures['tanh'] = {}\n",
    "\n",
    "        #epoch = 0\n",
    "\n",
    "        cur_dir = 'activations'\n",
    "        if not os.path.exists(cur_dir):\n",
    "            print(\"Directory %s not found\" % cur_dir)\n",
    "\n",
    "        # Load files saved during each epoch, and compute MI measures of the activity in that epoch\n",
    "        print('*** Doing %s ***' % cur_dir)\n",
    "        for epochfile in sorted(os.listdir(cur_dir)):\n",
    "            if not epochfile.startswith('epoch'):\n",
    "                continue\n",
    "\n",
    "            fname = cur_dir + \"/\" + epochfile\n",
    "            with open(fname, 'rb') as f:\n",
    "                d = pickle.load(f)\n",
    "\n",
    "            epoch = d[1]\n",
    "            d = d[0]\n",
    "            if epoch in measures['relu']:# Skip this epoch if its already been processed\n",
    "                continue                # this is a trick to allow us to rerun this cell multiple times)\n",
    "\n",
    "            if epoch > MAX_EPOCHS:\n",
    "                continue\n",
    "\n",
    "            print(\"Doing\", fname)\n",
    "\n",
    "            num_layers = len(d)\n",
    "\n",
    "            if self.PLOT_LAYERS is None:\n",
    "                PLOT_LAYERS = []\n",
    "                for lndx in range(num_layers):\n",
    "                    PLOT_LAYERS.append(lndx)\n",
    "\n",
    "            cepochdata = defaultdict(list)\n",
    "            for lndx in range(num_layers):\n",
    "                activity = d[lndx]\n",
    "\n",
    "                # Compute marginal entropies\n",
    "                h_upper = entropy_func_upper(activity)\n",
    "                if self.DO_LOWER:\n",
    "                    #h_lower = entropy_func_lower([activity,])[0]\n",
    "                    h_lower = entropy_func_lower(activity)\n",
    "\n",
    "                # Layer activity given input. This is simply the entropy of the Gaussian noise\n",
    "                hM_given_X = kde.kde_condentropy(activity, self.noise_variance)\n",
    "\n",
    "                # Compute conditional entropies of layer activity given output\n",
    "                hM_given_Y_upper=0.\n",
    "                for i in range(10):\n",
    "                    hcond_upper = entropy_func_upper(activity[saved_labelixs[i],:])\n",
    "                    hM_given_Y_upper += labelprobs[i] * hcond_upper\n",
    "\n",
    "                if self.DO_LOWER:\n",
    "                    hM_given_Y_lower=0.\n",
    "                    for i in range(10):\n",
    "                        hcond_lower = entropy_func_lower(activity[saved_labelixs[i],:])\n",
    "                        hM_given_Y_lower += labelprobs[i] * hcond_lower\n",
    "\n",
    "                cepochdata['MI_XM_upper'].append( nats2bits * (h_upper - hM_given_X) )\n",
    "                cepochdata['MI_YM_upper'].append( nats2bits * (h_upper - hM_given_Y_upper) )\n",
    "                cepochdata['H_M_upper'  ].append( nats2bits * h_upper )\n",
    "\n",
    "                pstr = 'upper: MI(X;M)=%0.3f, MI(Y;M)=%0.3f, h_upper=%0.3f, hM_given_X=%0.3f, hM_given_Y_upper=%0.3f ' % (cepochdata['MI_XM_upper'][-1], cepochdata['MI_YM_upper'][-1], h_upper, hM_given_X, hM_given_Y_upper)\n",
    "                if self.DO_LOWER:  # Compute lower bounds\n",
    "                    cepochdata['MI_XM_lower'].append( nats2bits * (h_lower - hM_given_X) )\n",
    "                    cepochdata['MI_YM_lower'].append( nats2bits * (h_lower - hM_given_Y_lower) )\n",
    "                    cepochdata['H_M_lower'  ].append( nats2bits * h_lower )\n",
    "                    pstr += ' | lower: MI(X;M)=%0.3f, MI(Y;M)=%0.3f' % (cepochdata['MI_XM_lower'][-1], cepochdata['MI_YM_lower'][-1])\n",
    "\n",
    "                if self.DO_BINNED: # Compute binner estimates\n",
    "                    binxm, binym = simplebinmi.bin_calc_information2(saved_labelixs, activity, 0.5)\n",
    "                    cepochdata['MI_XM_bin'].append( nats2bits * binxm )\n",
    "                    cepochdata['MI_YM_bin'].append( nats2bits * binym )\n",
    "                    pstr += ' | bin: MI(X;M)=%0.3f, MI(Y;M)=%0.3f' % (cepochdata['MI_XM_bin'][-1], cepochdata['MI_YM_bin'][-1])\n",
    "\n",
    "                print('- Layer %d %s' % (lndx, pstr) )\n",
    "\n",
    "            measures['relu'][epoch] = cepochdata\n",
    "             \n",
    "            #epoch += 1\n",
    "        \n",
    "        with open('MI_cnn', 'wb') as f:\n",
    "            pickle.dump(measures, f, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "    def plot(self, DO_SAVE=False):\n",
    "        self.DO_SAVE = DO_SAVE\n",
    "        with open('MI_cnn', 'rb') as f:\n",
    "            measures = pickle.load(f)\n",
    "            \n",
    "        COLORBAR_MAX_EPOCHS = self.n_epochs\n",
    "        infoplane_measure = 'upper'\n",
    "        PLOT_LAYERS = [0,1,2,3,4] #Which layers to plot\n",
    "        \n",
    "        max_epoch = max( (max(vals.keys()) if len(vals) else 0) for vals in measures.values())\n",
    "        sm = plt.cm.ScalarMappable(cmap='gnuplot', norm=plt.Normalize(vmin=0, vmax=COLORBAR_MAX_EPOCHS))\n",
    "        sm._A = []\n",
    "\n",
    "        fig=plt.figure(figsize=(10,5))\n",
    "        for actndx, (activation, vals) in enumerate(measures.items()):\n",
    "            epochs = sorted(vals.keys())\n",
    "            if not len(epochs):\n",
    "                continue\n",
    "            plt.subplot(1,2,actndx+1)    \n",
    "            for epoch in epochs:\n",
    "                c = sm.to_rgba(epoch)\n",
    "                xmvals = np.array(vals[epoch]['MI_XM_'+infoplane_measure])[PLOT_LAYERS]\n",
    "                ymvals = np.array(vals[epoch]['MI_YM_'+infoplane_measure])[PLOT_LAYERS]\n",
    "\n",
    "                plt.plot(xmvals, ymvals, c=c, alpha=0.1, zorder=1)\n",
    "                plt.scatter(xmvals, ymvals, s=20, facecolors=[c for _ in PLOT_LAYERS], edgecolor='none', zorder=2)\n",
    "\n",
    "            plt.ylim([0, 3.5])\n",
    "            plt.xlim([0, 14])\n",
    "            plt.xlabel('I(X;M)')\n",
    "            plt.ylabel('I(Y;M)')\n",
    "            plt.title(activation)\n",
    "\n",
    "        cbaxes = fig.add_axes([1.0, 0.125, 0.03, 0.8]) \n",
    "        plt.colorbar(sm, label='Epoch', cax=cbaxes)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        if self.DO_SAVE:\n",
    "            plt.savefig('plots/' + DIR_TEMPLATE % ('infoplane_'+ARCH),bbox_inches='tight') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plate = MI_Plate() #Create class object\n",
    "Plate.save_activations() #save activations if they weren't saved already\n",
    "Plate.compute_MI() #compute and save MIs if they weren't saved before\n",
    "Plate.plot() #plot "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
